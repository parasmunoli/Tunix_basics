{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ada966",
   "metadata": {},
   "source": [
    "# Experiment 1:\n",
    "# Supervised Fine-Tuning with LoRA on GSM8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf19cea",
   "metadata": {},
   "source": [
    "### Loading GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ac27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load GSM8K dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")\n",
    "\n",
    "# Let's look at a sample\n",
    "sample = train_data[0]\n",
    "print(f\"\\nSample Question: {sample['question']}\")\n",
    "print(f\"Sample Answer: {sample['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c246de2",
   "metadata": {},
   "source": [
    "### Preparing the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad79082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_gsm8k_for_training(example): # Formating GSM8K examples into instruction-response pairs\n",
    "    instruction = f\"Solve this math problem step by step:\\n{example['question']}\"\n",
    "    response = example['answer']\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "# Process the dataset\n",
    "formatted_train = [format_gsm8k_for_training(ex) for ex in train_data]\n",
    "formatted_test = [format_gsm8k_for_training(ex) for ex in test_data]\n",
    "\n",
    "print(f\"Formatted {len(formatted_train)} training examples\")\n",
    "print(f\"\\nExample formatted input:\")\n",
    "print(formatted_train[0]['instruction'])\n",
    "print(f\"\\nExpected output:\")\n",
    "print(formatted_train[0]['response'][:100] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8aa9cb",
   "metadata": {},
   "source": [
    "### The Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tunix.trainers import PeftTrainer\n",
    "from flax import linen as nn\n",
    "from transformers import FlaxAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load LLM model and tokenizer\n",
    "model_name = \"google/gemma-2b\"\n",
    "model = FlaxAutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure LoRA parameters\n",
    "lora_config = {\n",
    "    \"rank\": 8,\n",
    "    \"alpha\": 16,\n",
    "    \"dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "}\n",
    "\n",
    "# Initialize the PeftTrainer (Only for supervised learning.)\n",
    "trainer = PeftTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    lora_config=lora_config,\n",
    "    learning_rate=2e-4,\n",
    "    num_epochs=3,\n",
    "    batch_size=4,\n",
    "    max_length=512,\n",
    "    warmup_steps=100\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "metrics = trainer.train(formatted_train[:1000])  # Using subset for demo\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"  Initial loss: {metrics['initial_loss']:.4f}\")\n",
    "print(f\"  Final loss: {metrics['final_loss']:.4f}\")\n",
    "print(f\"  Training time: {metrics['training_time_minutes']:.1f} minutes\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_checkpoint(\"./gemma-2b-gsm8k-lora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
